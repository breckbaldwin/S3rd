---
title: "A New Funding Strategy for Scientific Software and an Experiment"
subtitle: ""
author: "Breck Baldwin, breckbaldwin@gmail.com"
email: "breckbaldwin@gmail.com"
affiliation: "Safety 3rd"


output:
 bookdown::html_document2:
#   includes:
#     in_header: _html/ga.html 
     
#bibliography: citations.bib
#csl: american-medical-association.csl
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	comment = NA
)
```

__Challenge__

The depth, breadth and impact of scientific open source software exceeded any expectations that one might have had at the start of the millennium. We enjoy high quality, largely free, state-of-the-art technology all of which came to be due to a nearly magical confluence of cultural changes in software developers, backing by huge technology companies and the development of open source behavioral conventions that coordinate disparate developers towards a common goal. Even Microsoft[^0] supports and releases open source software now, inconceivable in year 1999.

[^0]:2004 was the first release of an open source Windows installer, https://en.wikipedia.org/wiki/Microsoft_and_open_source

Nobody planned this, it just came to be and we are very fortunate to have it. However the heady days of software creation are meeting the realities of middle age which are far less 'fun' and the work has become more like actual work involving maintenance, support and shoring up all the fast fun work with re-engineered, re-conceptualized versions of what already exists. This is work-work that is best handled by people paid to do the work-work, not volunteers.

A very welcome development is that scientific funding agencies are recognizing that all this software could use some resources to help maintain it, e.g., NASA's recent call for infrastructure funding with the TOPS program. I have been executive director of a major open source software organization, Stan, and if you have not heard of Stan then you hopefully have heard of NumFOCUS of which Stan is a member along with NumPy, SciPy, Julia and others. This position paper comes from what I thought was a more ideal way to allocate limited resources for scientific software.

So I am arguing that funding process needs to evolve as scientific software has evolved. Where do we stand now? Typical reviewers expect novelty in proposals, lean hard on the reputations of the principle investigator/team and then, if awarded, awards less funds than requested. This is a disaster. Addressing each in turn:

Innovation: Funders/reviewers, even for infrastructure proposals, want something new. In my experience, 'infrastructure' in an RFP (request for proposals) means build new infrastructure, not 'stay the course'. Bigger, better, faster even if it is not a new idea. This is not what maturing open source projects need. As time advances software needs to be redesigned, perhaps made smaller, documentation needs to be rewritten--all of this is frankly boring but vital.

Reputation: The creators/maintainers of much open source scientific software are relatively junior, not famous and often don't have strong publication records. Teams of them work from soft money jobs with tremendous responsibility and impact but that is not reflected in Nature publications.

Volatile award amounts: The traditional 10-30% haircut upon award hurts small organizations and any efforts at diversity expressed in the funding application. A 90% paid position is not a position anymore if that is the only source of funding, a diversity initiative will be the first thing cut if all funds don't show up. There is no 'other source' of funding for these projects typically.

__Opportunity__

The opportunity here revolves around fundamentally changing how funding decisions get made. The 'Intellectual Merit' and corresponding translation into whatever DOE/DARPA/NASA/NIH etc... calls it has to go. In place of innovation I suggest focusing on anti-innovation by evaluating past and current metrics with the future being "more of the same". In more detail I suggest metrics like: 1) How much is the software used as quantified by citation count, downloads, forum use or breathless letters of support.
2) Evidence of an active development community--pull request processing, governance, forums, support.
3) Evidence that project knows how to spend money in support of project goals.
Also, define the merit of the proposal to be well defined/easy to compute hurdles that an organization can aspire to achieve in checklist like fashion-- "PR ageing less than 10 days on average, check, average response to forum post 36 hours, check, diversity program in place, check..." Defining such hurdles will be a positive influence on scientific software development if done well, but they must be backed by actual funding decisions.

Stop funding top N selection: Algorithmic policies impose numerical scores whose goal is to establish relative rank of all proposals with some wiggle room for program manager judgement. If reviewers/program managers were infallible this would be a great strategy but I question the oracular properties of this arrangement. Any total order selection process reinforces biases, easy quid-pro-quo understandings and pure reputation based funded-for-being-funded feedback loops.

Start randomly selecting from N above a threshold: We could send all reviewers off to Baldwin's re-education camp for unbiased evaluation, or... just have reviewers eliminate proposals that seem very unlikely to succeed and fund those that remain. I suggest the awardees be chosen by random draw if there are not sufficient funds. Also, no funding reductions to 'stretch' the budget. 

__Maturity__

Why now? Obviously federal funders have figured out that infrastructure funding for all this free software might be a good idea. The graph below is a very simple simulation demonstrating the consequences of selection of the top N proposals based on reviewer scores compared to randomly selecting N proposals that are above a threshold. The simulation can be altered and experimented with at: https://blooming-lake-98194.herokuapp.com/.

<img src="file:/home/breck/git/S3rd/presentations/DOE2021/simRun.png" style="height:200px;align:center;" /> 

Resources are reported on the y axis with 1 unit per award, an award increases reputation of a project by .1 and the score is reputation + constant merit (.2) + random noise. The 'Top N selection' strategy are shown by lines which concentrates resources to past awardees given their reputation increment. The 'Random N selection' strategy, dots, works exactly the same except random awardees are selected that are above threshold of .3. The latter spreads resources much more evenly. 

__An Experiment__

These position papers are supposed to be the about the science of scientific software, so I'll suggest an experiment. DOE's next call for proposals should award 50% based on the standard funding process and 50% with a random draw from a pool of competent submissions. As much as it pains me as Bayesian, we have a perfect null hypothesis test of the oracle of current practices against the power of randomness.  
